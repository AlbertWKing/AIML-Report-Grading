{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutput(last_hidden_state=tensor([[[ 1.1810, -0.4073,  0.9986,  ..., -0.7445,  0.0380, -0.5510],\n",
      "         [ 1.6172, -0.6785,  1.6932,  ..., -0.8216, -0.2387, -0.6187],\n",
      "         [ 2.0840, -0.5496,  1.3313,  ..., -0.7791,  0.1698, -0.3950],\n",
      "         ...,\n",
      "         [ 0.2879, -0.1813,  1.2631,  ..., -0.2022,  0.4699,  0.5535],\n",
      "         [ 0.6069, -0.1943,  0.7584,  ..., -0.5106, -0.4027, -0.4910],\n",
      "         [ 1.0183, -0.8215,  0.9088,  ..., -0.8094,  0.8372, -0.2027]]]), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "# from HuggingFace https://huggingface.co/distilbert/distilbert-base-uncased-distilled-squad\n",
    "\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import torch\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased-distilled-squad')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-cased-distilled-squad')\n",
    "question, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n",
    "\n",
    "inputs = tokenizer(question, text, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a nice puppet\n"
     ]
    }
   ],
   "source": [
    "# copilot modified for text output.\n",
    "\n",
    "from transformers import DistilBertTokenizer, DistilBertForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased-distilled-squad')\n",
    "model = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased-distilled-squad')\n",
    "\n",
    "question, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n",
    "\n",
    "inputs = tokenizer(question, text, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "\n",
    "answer_start_index = torch.argmax(start_logits)\n",
    "answer_end_index = torch.argmax(end_logits)\n",
    "\n",
    "predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
    "answer = tokenizer.decode(predict_answer_tokens)\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "\n",
    "# Load the Word document\n",
    "doc = Document('./example.docx')\n",
    "\n",
    "# Extract text\n",
    "text = []\n",
    "for paragraph in doc.paragraphs:\n",
    "    text.append(paragraph.text)\n",
    "\n",
    "# Combine paragraphs into a single string (optional)\n",
    "full_text = '\\n'.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7651"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "citric acid\n"
     ]
    }
   ],
   "source": [
    "question, text = \"What is the identity of the unknown acid?\", full_text[:1000]\n",
    "\n",
    "inputs = tokenizer(question, text, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "\n",
    "answer_start_index = torch.argmax(start_logits)\n",
    "answer_end_index = torch.argmax(end_logits)\n",
    "\n",
    "predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
    "answer = tokenizer.decode(predict_answer_tokens)\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Define the directory you want to search in\n",
    "directory = './data'\n",
    "\n",
    "# Use glob to find all .docx files in the directory\n",
    "docx_files = glob.glob(os.path.join(directory, '*.docx'))\n",
    "print(type(docx_files))\n",
    "\n",
    "# Print the list of .docx files\n",
    "# for file in docx_files:\n",
    "#     print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\"who wrote this?\", 'what is the molar mass of the unknown?', \"what is the identity of the unknown?\", 'is this well written?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in docx_files:\n",
    "    \n",
    "    answers = {file:{}}\n",
    "    \n",
    "    # Load the Word document\n",
    "    doc = Document(f'{file}')\n",
    "\n",
    "    # Extract text\n",
    "    text = []\n",
    "    for paragraph in doc.paragraphs:\n",
    "        text.append(paragraph.text)\n",
    "\n",
    "    # Combine paragraphs into a single string (optional)\n",
    "    full_text = '\\n'.join(text)\n",
    "\n",
    "    for question in questions:\n",
    "        \n",
    "        asking, text = question, full_text[:1000]\n",
    "\n",
    "        inputs = tokenizer(asking, text, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        start_logits = outputs.start_logits\n",
    "        end_logits = outputs.end_logits\n",
    "\n",
    "        answer_start_index = torch.argmax(start_logits)\n",
    "        answer_end_index = torch.argmax(end_logits)\n",
    "\n",
    "        predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
    "        answer = tokenizer.decode(predict_answer_tokens)\n",
    "\n",
    "        answers[f'{file}'][question] = answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "story1 = \"\"\"The Adventure of Bob's Day\n",
    "Bob woke up in the morning when his alarm went off and he got out of bed and he was really tired. He went to the bathroom and brushed his teeth with toothpaste that was minty fresh and then he went downstairs. The kitchen was where he went next and he made some breakfast which was eggs and toast and orange juice.\n",
    "Bob worked at an office building downtown where lots of other people worked too and he had to drive there in his car which was blue. Traffic was really bad and horrible and terrible and it made Bob feel very angry and mad. When he finally got to work, his boss Tom was waiting for him and Tom was mad because Bob was late.\n",
    "\"Your late Bob,\" said Tom angrily and mad. \"I know and I'm sorry because of the traffic being bad,\" Bob replied back to Tom in response. Bob sat at his desk and typed on his computer all day long and it was really boring and not fun at all and he wished he could go home.\n",
    "Finally it was time to go home and Bob was happy and glad. But then something crazy happened - there was a alien spaceship in the parking lot!!! The aliens were green and had three eyes and antennae on their heads that were long. \"Take me to your leader!\" they said to Bob loudly. But Bob just ran away really fast and got in his car and drove home really quick.\n",
    "When Bob got home he was scared and frightened but happy to be alive and ok. He decided to never tell anyone about the aliens because they wouldn't believe him anyway. The end.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'./data/Pendleton.docx': {'who wrote this?': '<s>',\n",
       "  'what is the molar mass of the unknown?': ' 205.0 g/mol',\n",
       "  'what is the identity of the unknown?': '<s>',\n",
       "  'is this well written?': 'By counting the number of moles of acid that reacted with the NaOH and comparing it to the mass of the acid, the molar mass of the unknown acid was ascertained.. The mean molar mass of the unidentified acid was 205.0 g/mol, with a margin of error ascribed to systematic differences in handling and titration volume measurements'}}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]\n"
     ]
    }
   ],
   "source": [
    "question, text = \"Summarize the story.\", story1\n",
    "\n",
    "inputs = tokenizer(question, text, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "\n",
    "answer_start_index = torch.argmax(start_logits)\n",
    "answer_end_index = torch.argmax(end_logits)\n",
    "\n",
    "predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
    "answer = tokenizer.decode(predict_answer_tokens)\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32f4769b9395428c9b96d1ed746efd26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da42c4551bc04dbcbf99d809c0ecd537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dd419d204b140fa9fbd5a92fe20e41b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/AIML/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8f5eb4974f9451799abe5dceee65578",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/694 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcd43562242c4d9db12ad1ff161ce26b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/597M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/AIML/lib/python3.8/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 0\n"
     ]
    }
   ],
   "source": [
    "from transformers import LongformerTokenizer, LongformerForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load pre-trained Longformer model and tokenizer\n",
    "model_name = 'allenai/longformer-base-4096'\n",
    "tokenizer = LongformerTokenizer.from_pretrained(model_name)\n",
    "model = LongformerForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Function to analyze a string\n",
    "def analyze_string(text):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding='max_length', max_length=4096)\n",
    "    \n",
    "    # Perform the classification\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Get the predicted class (assuming binary classification)\n",
    "    predicted_class = torch.argmax(outputs.logits, dim=1).item()\n",
    "    \n",
    "    return predicted_class\n",
    "\n",
    "# Example usage\n",
    "text = \"This lab report is very detailed and well-written.\"\n",
    "result = analyze_string(text)\n",
    "print(\"Predicted class:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/AIML/lib/python3.8/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of LongformerForQuestionAnswering were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import LongformerForQuestionAnswering\n",
    "\n",
    "# Load pre-trained Longformer model and tokenizer\n",
    "model_name = 'allenai/longformer-base-4096'\n",
    "tokenizer = LongformerTokenizer.from_pretrained(model_name)\n",
    "model = LongformerForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "# Function to ask a question\n",
    "def ask_question(context, question):\n",
    "    # Tokenize the input context and question\n",
    "    inputs = tokenizer(context, question, return_tensors='pt', truncation=True, padding='max_length', max_length=4096)\n",
    "    \n",
    "    # Perform the question answering\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Get the answer\n",
    "    start_scores, end_scores = outputs.start_logits, outputs.end_logits\n",
    "    start_index = torch.argmax(start_scores)\n",
    "    end_index = torch.argmax(end_scores) + 1\n",
    "\n",
    "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][start_index:end_index]))\n",
    "    \n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: \n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "context = \"The unknown is sodium chloride.\"\n",
    "question = \"What is the molar mass of the unknown?\"\n",
    "answer = ask_question(full_text, question)\n",
    "print(\"Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "\n",
    "# Load the Word document\n",
    "doc = Document('./example.docx')\n",
    "\n",
    "# Extract text\n",
    "text = []\n",
    "for paragraph in doc.paragraphs:\n",
    "    text.append(paragraph.text)\n",
    "\n",
    "# Combine paragraphs into a single string (optional)\n",
    "full_text = '\\n'.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "# Load pre-trained RoBERTa model and tokenizer\n",
    "model_name = 'deepset/roberta-base-squad2'\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "model = RobertaForQuestionAnswering.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:  citric acid\n"
     ]
    }
   ],
   "source": [
    "# Function to ask a question\n",
    "def ask_question(context, question):\n",
    "    # Tokenize the input context and question\n",
    "    inputs = tokenizer(question, context, return_tensors='pt', truncation=True, padding='max_length', max_length=512)\n",
    "    \n",
    "    # Perform the question answering\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Get the answer\n",
    "    start_scores, end_scores = outputs.start_logits, outputs.end_logits\n",
    "    start_index = torch.argmax(start_scores)\n",
    "    end_index = torch.argmax(end_scores) + 1\n",
    "\n",
    "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][start_index:end_index]))\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# Example usage\n",
    "context = \"The unknown was sodium chloride.\"\n",
    "question = \"What is the unknown?\"\n",
    "answer = ask_question(full_text, question)\n",
    "print(\"Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification result: Well Written\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline('zero-shot-classification', model='facebook/bart-large-mnli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification result: Well Written\n"
     ]
    }
   ],
   "source": [
    "def classify_report(report_text):\n",
    "    labels = [\"Well Written\", \"Poorly Written\"]\n",
    "    result = classifier(report_text, candidate_labels=labels)\n",
    "    return result['labels'][0]\n",
    "\n",
    "# Example usage\n",
    "report_text = \"This lab report describes the experimental procedure clearly and concisely.\"\n",
    "result = classify_report(story1)\n",
    "print(\"Classification result:\", result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
